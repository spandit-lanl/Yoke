"""Module containing classes for *variable embeddings*, *position embeddings*,
*temporal embeddings*.

"""

from functools import lru_cache

import numpy as np

import torch
import torch.nn as nn


def get_1d_sincos_pos_embed_from_grid(embed_dim, position):
    """1D Sine/Cosine embedding.

    Args:
        embed_dim (int): Must be divisible by 2. Output dimension, D, for each
                         position
        position (np.array): Size (M, 1) array of 1D-positions
                             to be encoded.

    Returns:
        emb (np.array): Size (M, D) array consisting of the outer product of
                        position-index values and a phase passed through sine
                        and cosine. The first D/2 columns are passed through
                        Sine, the second D/2 columns are passed through Cosine.
    
    """
    assert embed_dim % 2 == 0

    # omega = 10000^(-w) with w=(2/D)*[0, 1, 2, ..., (D/2) - 1]
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (D/2,)

    # Flatten position array
    position = position.reshape(-1)  # (M,)

    # Compute outer product of position vector and omega vector:
    #
    # out = position(M, 1) x omega(1, D/2) or out(i, j) = position(i) x omega(j)
    out = np.einsum("m,d->md", position, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out)  # (M, D/2)
    emb_cos = np.cos(out)  # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)

    return emb


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    """2D Sine/Cosine embedding using grid array.

    Args:
        embed_dim (int): Must be divisible by 2. Output dimension, D, for each
                         position
        grid (np.array): Size (2, *singleton dimensions*, M) array of 2D-positions
                         to be encoded. Singleton dimensions are ignored through
                         flattening of the grid[0] and grid[1] sub-arrays.

    Returns:
        emb (np.array): Size (2, M, D) array with each (1, M, D)-subarray
                        consisting of the outer product of
                        position-index values and a phase passed through sine
                        and cosine. The first D/2 columns are passed through
                        Sine, the second D/2 columns are passed through Cosine.

    """
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (M, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (M, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1)  # (M, D)

    return emb


def get_2d_sincos_pos_embed(embed_dim,
                            grid_size_h,
                            grid_size_w,
                            cls_token=False):
    """2D Sine/Cosine embedding on a index-grid of specified dimensions.

    Args:
        embed_dim (int): Must be divisible by 2. Output dimension, D, for each
                         position
        grid_size_h (int): Index grid height.
        grid_size_w (int): Index grid width.
        cls_token (bool): Whether or not a class-token embedding is appended.

    Returns:
        pos_embed (np.array): [grid_size_h*grid_size_w, embed_dim] OR
                              [1+grid_size*grid_size, embed_dim] with class token

    """
    assert embed_dim % 2 == 0

    W = grid_size_w
    H = grid_size_h
    grid_h = np.arange(H, dtype=np.float32)
    grid_w = np.arange(W, dtype=np.float32)
    # Create list of arrays for each grid dimension.
    # grid[0] = repeat(row(0:grid_w))
    # grid[1] = repeat(col(0:grid_h))
    grid = np.meshgrid(grid_w, grid_h)  # grid[i].shape=(W, H), i=1,2
    grid = np.stack(grid, axis=0)  # (2, W, H)

    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)

    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed],
                                   axis=0)

    return pos_embed


class ClimaX_VarEmbed(nn.Module):
    """Variable encoding/embedding to denote which variable each token belongs
    to. Helps in tracking variables through variable aggregation layer. Prior
    to variable aggregation, embedding is added to parallel patch embedding
    output to tag variable entries.

    This embedding consists of learnable weights but the weights are
    initialized with a sin/cos embedding.

    Based on the paper, **ClimaX: A foundation model for weather and
    climate.**
    
    Args:
        default_vars (list): list of default variables to be used for training
        embed_dim (int): Embedding dimension

    """

    def __init__(self,
                 default_vars,
                 embed_dim):

        super().__init__()

        self.default_vars = default_vars
        self.embed_dim = embed_dim

        # var_map is dictionary {var_name_str : var_idx_int}
        #
        # var_embed is (1, max_vars, embed_dim) tensor
        self.var_embed, self.var_map = self.create_var_embedding(embed_dim)

        # Initialize var_embed with sincos embedding
        self.initialize_weights()

    def create_var_embedding(self, dim):
        var_embed = nn.Parameter(torch.zeros(1, len(self.default_vars), dim),
                                 requires_grad=True)
        # TODO: create a mapping from var --> idx
        var_map = {}
        idx = 0

        for var in self.default_vars:
            var_map[var] = idx
            idx += 1

        return var_embed, var_map

    def initialize_weights(self):
        var_embed = get_1d_sincos_pos_embed_from_grid(self.var_embed.shape[-1],
                                                      np.arange(len(self.default_vars)))
        self.var_embed.data.copy_(torch.from_numpy(var_embed).float().unsqueeze(0))

    @lru_cache(maxsize=None)
    def get_var_ids(self, vars, device):
        ids = np.array([self.var_map[var] for var in vars])
        return torch.from_numpy(ids).to(device)

    def get_var_emb(self, var_emb, vars):
        ids = self.get_var_ids(vars, var_emb.device)
        return var_emb[:, ids, :]

    def forward(self, x, vars):
        # The input tensor is shape:
        #  (B, V, L, D)=(B, NumVars, NumTokens, embed_dim)
        variables = tuple(vars)

        # Variable embedding is (1, V, D)
        var_embed = self.get_var_emb(self.var_embed, variables)

        # Unsqueeze a dimension after variable dimension...
        #   var_embed -> (1, V, 1, D)
        x = x + var_embed.unsqueeze(2)  # B, V, L, D

        return x


class ClimaX_PosEmbed(nn.Module):
    """Position encoding/embedding to help track where each patch embedding is
    located in relation to the others. After variable aggregation, embedding is
    added to patch tokens.

    This embedding consists of learnable weights but the weights are
    initialized with a 2D-sine/cosine embedding.

    NOTE: Entries for image_size and patch_size should divide eachother evenly.

    Based on the paper, **ClimaX: A foundation model for weather and
    climate.**
    
    Args:
        embed_dim (int): Embedding dimension, must be divisible by 2.
        patch_size (tuple): Height and width of patches
        image_size (tuple): Height and width of original image
        num_patches (int): Number of patches after initial patch embedding.

    """

    def __init__(self,
                 embed_dim,
                 patch_size,
                 image_size,
                 num_patches):

        super().__init__()

        self.embed_dim = embed_dim
        self.patch_size = patch_size
        self.image_size = image_size
        self.num_patches = num_patches

        self.pos_embed = nn.Parameter(torch.zeros(1,
                                                  self.num_patches,
                                                  self.embed_dim),
                                      requires_grad=True)

        self.initialize_weights()

    def initialize_weights(self):
        # Initialize position embedding with Sine/Cosine grid
        pos_embed = get_2d_sincos_pos_embed(
            self.pos_embed.shape[-1],
            int(self.image_size[0] / self.patch_size[0]),
            int(self.image_size[1] / self.patch_size[1]),
            cls_token=False,
        )
        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))

    def forward(self, x):
        # The input tensor is shape:
        #  (B, L, D)=(B, NumTokens[i.e. Patches], embed_dim)

        x = x + self.pos_embed

        return x


class RelativePositionEmbed(nn.Module):
    """Relative spatial encoding/embedding used in the SWIN windowed
    multi-headed self-attention layers. In this specialized version we set up a
    relative position embedding for a rectangular image.

    
    Args:
        window_size (int, int): Embedding dimension, must be divisible by 2.

    """

    def __init__(self, window_size: (int, int) = (8, 4)):

        super().__init__()

        # Set window size
        self.window_size = window_size

        # Since these weights are an nn.Parameter they will be updated during
        # training.
        self.pos_embeddings = nn.Parameter(torch.randn(2 * self.window_size[0] - 1,
                                                       2 * self.window_size[1] - 1))

        # Create array of indices for each point in a window of size
        # (wh, ww), 'indices.shape = (wh*ww, 2)'
        idx_array = np.array([[x, y] for x in range(self.window_size[0]) for y in range(self.window_size[1])])
        self.indices = torch.tensor(idx_array)

        # Using 'None' creates an extra singleton dimension,
        # i.e. self.indices[None, :, :] has dimension (1, wh*ww, 2).
        # Therefore, self.relative_indices has size (wh*ww, wh*ww, 2) and
        # the entry self.relative_indices[i, j, :, :] is the (delta-rowIDX, delta-colIDX)
        # differences between the i and j points in the window.
        self.relative_indices = self.indices[None, :, :] - self.indices[:, None, :]

        # The previous range of self.relative_indices entries was [-wh+1, wh-1]
        # for the row-indices and [-ww+1, ww-1] for the column-indices. By
        # adding wh-1 to each entry in the first channel and ww-1 to each entry
        # in the second channel the new ranges are [0, (2*wh-1)-1] and [0,
        # (2*ww-1)-1], respectively. This ensures the `relative_indices` array
        # can be used as indices in self.pos_embeddings
        self.relative_indices[:, :, 0] += self.window_size[0] - 1
        self.relative_indices[:, :, 1] += self.window_size[1] - 1

    def forward(self, x):
        # The input tensor is shape:
        #  (B, H, Hw, Ww, wh*ww, wh*ww)
        #
        # Here:
        #    B = batch size
        #    H = number of heads
        #    Hw = Height of window grid
        #    Ww = Width of window grid
        #    wh = window height
        #    ww = window_width
        #
        # In WindowedMSA, the input `x` represents the attention weights between
        # every pair of patches/tokens within each of the Hw x Ww
        # windows. There are wh*ww patches/tokens within each window so (wh*ww,
        # wh*ww) pairs so shape(x) = (B, H, Hw, Ww, wh*ww, wh*ww)

        # Get embedding weights between each pair of window positions.
        rel_pos_embedding = self.pos_embeddings[self.relative_indices[:, :, 0],
                                                self.relative_indices[:, :, 1]]

        # Using broadcasting the relative position embedding weights are added
        # to the last two dimensions of the attention weights for each batch,
        # head, and window.
        x = x + rel_pos_embedding

        return x


class ClimaX_TimeEmbed(nn.Module):
    """Temporal encoding/embedding to help track/tag each entry of a batch by
    it's corresponding lead time. After variable aggregation and position
    encoding, temporal encoding is added to patch tokens.

    This embedding consists of a single 1D linear embedding of lead-times per
    sample in the batch to the embedding dimension.

    NOTE: Entries for image_size and patch_size should divide eachother evenly.

    Based on the paper, **ClimaX: A foundation model for weather and
    climate.**
    
    Args:
        embed_dim (int): Embedding dimension, must be divisible by 2.

    Foward method args:
        lead_times (torch.Tensor): lead_times.shape = (B,). Forecasting lead
                                   times of each element of the batch.

    """

    def __init__(self, embed_dim):

        super().__init__()

        self.lead_time_embed = nn.Linear(1, embed_dim)

    def forward(self, x, lead_times: torch.Tensor):
        # The input tensor is shape:
        #  (B, L, D)=(B, NumTokens, embed_dim)

        # Add lead time embedding
        lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1))  # B, D
        lead_time_emb = lead_time_emb.unsqueeze(1)  # B, 1, D
        x = x + lead_time_emb  # B, L, D

        return x


if __name__ == '__main__':
    """Usage Example.

    """

    # Original: (B, C, H, W) = (3, 5, 128, 128)
    # After parallel-patch embedding: (B, V, L, D)
    x = torch.rand(3, 4, 64, 72)

    default_vars = ['cu_pressure',
                    'cu_density',
                    'cu_temperature',
                    'ss_pressure',
                    'ss_density',
                    'ss_temperature',
                    'r_vel',
                    'z_vel']

    embed_dim = 72
    patch_size = (16, 16)
    image_size = (128, 128)
    num_patches = 64

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    x = x.type(torch.FloatTensor).to(device)

    # Prior to variable aggregation: (B, V, L, D)
    var_emb_model = ClimaX_VarEmbed(default_vars=default_vars,
                                    embed_dim=embed_dim).to(device)

    print('Vaiable Encoding Input shape:', x.shape)
    print('Variable Encoding shape:',
          var_emb_model(x,
                        vars=['cu_density',
                              'ss_density',
                              'ss_temperature',
                              'r_vel']).shape)

    # After variable aggregation: (B, L, D)
    x = torch.rand(3, 64, 72)
    pos_emb_model = ClimaX_PosEmbed(embed_dim=embed_dim,
                                    patch_size=patch_size,
                                    image_size=image_size,
                                    num_patches=num_patches).to(device)

    print('Position Encoding input shape:', x.shape)
    print('Position Encoding shape:',
          pos_emb_model(x).shape)

    # After position encoding: (B, L, D)
    x = torch.rand(3, 64, 72)
    lead_times = torch.rand(3)
    print(lead_times)
    time_emb_model = ClimaX_TimeEmbed(embed_dim=embed_dim).to(device)

    print('Temporal Encoding input shape:', x.shape)
    print('Temporal Encoding shape:',
          time_emb_model(x, lead_times).shape)

    # Relative window embedding
    wh = 8
    ww = 5
    rel_emb_model = RelativePositionEmbed(window_size=(wh, ww)).to(device)
    print('Size of relative indices:', rel_emb_model.relative_indices.shape)
    print('Minimum of x-coord relative indices:', rel_emb_model.relative_indices[:, :, 0].min())
    print('Maximum of x-coord relative indices:', rel_emb_model.relative_indices[:, :, 0].max())
    print('Minimum of y-coord relative indices:', rel_emb_model.relative_indices[:, :, 1].min())
    print('Maximum of y-coord relative indices:', rel_emb_model.relative_indices[:, :, 1].max())

    # The input tensor is shape:
    #  (B, H, Hw, Ww, wh*ww, wh*ww)
    win_patches = torch.rand(3, 10, 8, 16, wh * ww, wh * ww)
    print('Size of relative-position embedding input:', win_patches.shape)
    print('Size of relative-position embedding output:',
          rel_emb_model(win_patches).shape)
